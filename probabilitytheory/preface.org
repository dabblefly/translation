** What is ‘safe’? 什么是"安全"?
We are not concerned here only with abstract issues of mathematics and logic. One of the main practical messages of this work is the great effect of prior information on the conclusions that one should draw from a given data set. Currently, much discussed issues, such as environmental hazards or the toxicity of a food additive, cannot be judged rationally if one looks only at the current data and ignores the prior information that scientists have about the phenomenon. This can lead one to overestimate or underestimate the danger.

在这里我们关心的不仅仅是抽象的数学和逻辑问题.本书有助于实际应用的是,先验信息显著影响了从给定数据集能得出什么样的结论.当今讨论的一些问题,比如环境污染,食品添加剂的毒性,如果只看数据而忽略科学家给出的相关先验信息,是难以得到合理结论的.只看数据将导致高估或低估危害程度.

A common error, when judging the effects of radioactivity or the toxicity of some substance, is to assume a linear response model without threshold (i.e. without a dose rate below which there is no ill effect). Presumably there is no threshold effect for cumulative poisons like heavy metal ions (mercury, lead), which are eliminated only very slowly, if at all. But for virtually every organic substance (such as saccharin or cyclamates), the existence of a finite metabolic rate means that there must exist a finite threshold dose rate, below which the substance is decomposed, eliminated, or chemically altered so rapidly that it causes no ill effects. If this were not true, the human race could never have survived to the present time, in view of all the things we have been eating. 

在判断某种物质的放射性或毒性时,一个常见的错误是预先假设了一个无门限值(低于指定值即为无毒性)的线性反应模型.例如对有毒性的重金属(水银,铅),如果视毒性累积是无门限的,且会被人体缓慢的代谢掉(如果能代谢的话).但几乎所有的有机物质(如糖精或甜蜜素),由于代谢率的原因,总是存在一个量值,在低于这个值的时候,该物质会被分解,排除,或快速的被化学降解导致不能产生毒性.如果不这样的话,在我们吃掉如此多样的食品时,人类就不能存活至今.

Indeed, every mouthful of food you and I have ever taken contained many billions of kinds of complex molecules whose structure and physiological effects have never been determined – and many millions of which would be toxic or fatal in large doses. We cannot doubt that we are daily ingesting thousands of substances that are far more dangerous than saccharin – but in amounts that are safe, because they are far below the various thresholds of toxicity. At present, there are hardly any substances, except some common drugs, for which we actually know the threshold.

实际上,你我每吃下的一口食物中,包含了数以亿计的各种复杂分子,其物理结构和作用从未曾被确定,而且其中百万计的物质可能在大剂量时是有毒的.我们无法怀疑每天吃进肚子里的上千中物质是远比糖精更大的风险,但从摄入量上看反而是安全的,因为其数量远低于其毒性的临界值.当今除了很少的一部分普通药品外,我们都不知道其临界值.

Therefore, the goal of inference in this field should be to estimate not only the slope of the response curve, but, far more importantly, to decide whether there is evidence for a threshold; and, if there is, to estimate its magnitude (the ‘maximum safe dose’). For example, to tell us that a sugar substitute can produce a barely detectable incidence of cancer in doses 1000 times greater than would ever be encountered in practice, is hardly an argument against using the substitute; indeed, the fact that it is necessary to go to kilodoses in order to detect any ill effects at all, is rather conclusive evidence, not of the danger, but of the safety, of a tested substance. A similar overdose of sugar would be far more dangerous, leading not to barely detectable harmful effects, but to sure, immediate death by diabetic coma; yet nobody has proposed to ban the use of sugar in food.

所以,在这些领域推导结论时,不应仅仅是估算反应曲线的斜率,更重要的是证明是否存在临界值,如果存在的话,如何估算临界值(最大的安全剂量).例如,已知一种甜性物质在超出常用剂量1000倍食用会导致癌症时,讨论其是否可以食用的问题并无多大意义.如果千倍的常用剂量才能检测到致病效果,与其说这证明了它是有害的,不如说证明了它是安全的.摄入如此多的糖就不是危不危险的问题了,这肯定会导致糖尿病患者直接死亡,但没人会要求禁止在食物中使用这类物质.

Kilodose effects are irrelevant because we do not take kilodoses; in the case of a sugar substitute the important question is: What are the threshold doses for toxicity of a sugar substitute and for sugar, compared with the normal doses? If that of a sugar substitute is higher, then the rational conclusion would be that the substitute is actually safer than sugar, as a food ingredient. To analyze one’s data in terms of a model which does not allow even the possibility of a threshold effect is to prejudge the issue in a way that can lead to false conclusions, however good the data. If we hope to detect any phenomenon, we must use a model that at least allows the possibility that it may exist.



We emphasize this in the Preface because false conclusions of just this kind are now not only causing major economic waste, but also creating unnecessary dangers to public health and safety. Society has only finite resources to deal with such problems, so any effort expended on imaginary dangers means that real dangers are going unattended. Even worse, the error is incorrectible by the currently most used data analysis procedures; a false premise built into a model which is never questioned cannot be removed by any amount of new data. Use of models which correctly represent the prior information that scientists have about the mechanism at work can prevent such folly in the future. 

Such considerations are not the only reasons why prior information is essential in inference; the progress of science itself is at stake. To see this, note a corollary to the preceding paragraph: that new data that we insist on analyzing in terms of old ideas (that is, old models which are not questioned) cannot lead us out of the old ideas. However many data we record and analyze, we may just keep repeating the same old errors, missing the same crucially important things that the experiment was competent to find. That is what ignoring prior information can do to us; no amount of analyzing coin tossing data by a stochastic model could have led us to the discovery of Newtonian mechanics, which alone determines those data.

Old data, when seen in the light of new ideas, can give us an entirely new insight into a phenomenon; we have an impressive recent example of this in the Bayesian spectrum analysis of nuclear magnetic resonance data, which enables us to make accurate quantitative determinations of phenomena which were not accessible to observation at all with the previously used data analysis by Fourier transforms. When a data set is mutilated (or, to use the common euphemism, ‘filtered’) by processing according to false assumptions, important information in it may be destroyed irreversibly. As some have recognized, this is happening constantly from orthodox methods of detrending or seasonal adjustment in econometrics. However, old data sets, if preserved unmutilated by old assumptions, may have a new lease on life when our prior information advances.
